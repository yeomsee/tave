{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression using Tensorflow 2.0"
      ],
      "metadata": {
        "id": "l8UYcZgfbG90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.array([[1., 1.],  [1., 2.], [2., 1.], \n",
        "              [3., 2.], [3., 3.], [2., 3.]], dtype=np.float32)\n",
        "\n",
        "y = np.array([[0.], [0.], [0.], [1.], [1.], [1.]], dtype=np.float32)"
      ],
      "metadata": {
        "id": "eKjIdc02kwgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "W = tf.Variable(tf.random.normal([2, 1], mean=0.0)) # Weight\n",
        "b = tf.Variable(tf.random.normal([1], mean=0.0)) # Bias\n",
        "\n",
        "print(\"Initial Weight : \", W.numpy())\n",
        "print(\"Initial Bias : \", b.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ3zKkWqg861",
        "outputId": "8a7e4b6f-687d-4503-f220-6d006acb436f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weight :  [[-1.3370554]\n",
            " [-0.2866867]]\n",
            "Initial Bias :  [-0.89793545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic classification\n",
        "def logistic_reg_output(X):\n",
        "    z = tf.matmul(X, W) + b\n",
        "    return 1 / (1 + tf.exp(-z))\n",
        "\n",
        "# cost function\n",
        "def cross_entropy(y_pred, y):\n",
        "    output = tf.reduce_mean(-tf.reduce_sum(y*tf.math.log(y_pred)\n",
        "                                           + (1-y)*tf.math.log(1-y_pred)))\n",
        "    return output"
      ],
      "metadata": {
        "id": "xeZoIMg0hoCK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 parameter\n",
        "lr = 0.001 # learning rate\n",
        "num_iter = 10000\n",
        "hist_loss = []\n",
        "hist_W = []\n",
        "hist_b = []\n",
        "idx_label = []"
      ],
      "metadata": {
        "id": "hMwSTC9UiEW6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in range(num_iter):\n",
        "    with tf.GradientTape() as tape: # 자동미분을 위한 GradientTape!\n",
        "        y_hat = logistic_reg_output(X)  \n",
        "        loss = cross_entropy(y_hat, y)\n",
        "\n",
        "    updated_W, updated_b = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(lr * updated_W)\n",
        "    b.assign_sub(lr * updated_b)\n",
        "    \n",
        "    hist_loss.append(loss.numpy())\n",
        "    if n%200==0:\n",
        "        print(n, '\\t', ' : Loss : ', loss.numpy())\n",
        "        hist_W.append(W.numpy())\n",
        "        hist_b.append(b.numpy())\n",
        "        idx_label.append(n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8C-5H1njH0M",
        "outputId": "fa5f10f4-9e8b-4001-d6bc-17006d61d8f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \t  : Loss :  15.859524\n",
            "200 \t  : Loss :  3.6249607\n",
            "400 \t  : Loss :  3.3557608\n",
            "600 \t  : Loss :  3.2314315\n",
            "800 \t  : Loss :  3.1162848\n",
            "1000 \t  : Loss :  3.008805\n",
            "1200 \t  : Loss :  2.908312\n",
            "1400 \t  : Loss :  2.814192\n",
            "1600 \t  : Loss :  2.7258904\n",
            "1800 \t  : Loss :  2.6429062\n",
            "2000 \t  : Loss :  2.5647902\n",
            "2200 \t  : Loss :  2.491137\n",
            "2400 \t  : Loss :  2.421582\n",
            "2600 \t  : Loss :  2.355798\n",
            "2800 \t  : Loss :  2.2934902\n",
            "3000 \t  : Loss :  2.2343912\n",
            "3200 \t  : Loss :  2.1782632\n",
            "3400 \t  : Loss :  2.124888\n",
            "3600 \t  : Loss :  2.074069\n",
            "3800 \t  : Loss :  2.0256276\n",
            "4000 \t  : Loss :  1.9794018\n",
            "4200 \t  : Loss :  1.935244\n",
            "4400 \t  : Loss :  1.8930186\n",
            "4600 \t  : Loss :  1.8526021\n",
            "4800 \t  : Loss :  1.8138813\n",
            "5000 \t  : Loss :  1.7767527\n",
            "5200 \t  : Loss :  1.7411207\n",
            "5400 \t  : Loss :  1.7068963\n",
            "5600 \t  : Loss :  1.6739991\n",
            "5800 \t  : Loss :  1.6423534\n",
            "6000 \t  : Loss :  1.6118896\n",
            "6200 \t  : Loss :  1.5825436\n",
            "6400 \t  : Loss :  1.5542547\n",
            "6600 \t  : Loss :  1.5269673\n",
            "6800 \t  : Loss :  1.5006294\n",
            "7000 \t  : Loss :  1.4751931\n",
            "7200 \t  : Loss :  1.4506125\n",
            "7400 \t  : Loss :  1.426846\n",
            "7600 \t  : Loss :  1.4038535\n",
            "7800 \t  : Loss :  1.3815982\n",
            "8000 \t  : Loss :  1.3600451\n",
            "8200 \t  : Loss :  1.3391614\n",
            "8400 \t  : Loss :  1.3189172\n",
            "8600 \t  : Loss :  1.2992829\n",
            "8800 \t  : Loss :  1.280232\n",
            "9000 \t  : Loss :  1.2617388\n",
            "9200 \t  : Loss :  1.2437791\n",
            "9400 \t  : Loss :  1.2263299\n",
            "9600 \t  : Loss :  1.2093707\n",
            "9800 \t  : Loss :  1.1928803\n"
          ]
        }
      ]
    }
  ]
}